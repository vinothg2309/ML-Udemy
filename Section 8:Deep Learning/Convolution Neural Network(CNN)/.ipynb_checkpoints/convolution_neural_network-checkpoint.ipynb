{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# rescale - Feature scaling - 255 is max value of scaling(0-255).\n",
    "# shear_range, zoom_range, horizontal_flip - to prevent overfitting\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64), # Final size of Image\n",
    "        batch_size=32,        # 32 training records(row) in 1 batch\n",
    "        class_mode='binary') # Binary or categorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#We shouldn't apply rest of t parameter as in training. Same as ML - fit_transform in training set \n",
    "#but only transform the test set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:Building CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters-32 different filter which detects vertical, horizontal edges, flip, blur etc\n",
    "# kernel_size - 3x3 Matrix for Feature detection. input_shape- Size of image as mentioned in preprocessing(64,64)& 3- RGB(since its colored image) \n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool_size - 2x2 pixel Sizing and strides move by 2 pixel. Refer TensorFlow-Udemy in /home/vinoth/Learning/Python\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a second convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_size - 3x3 Matrix for Feature detection. input_shape is required only when connecting 1st layer to input\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "# pool_size - 2x2 pixel Sizing and strides move by 2 pixel. Refer TensorFlow-Udemy in /home/vinoth/Learning/Python\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# units=1&activation='sigmoid' since its binary output\n",
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Training CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating weight of each neuron in hidden layer to reduce the loss over the epoch. Signal from Feature or dependent variable is forward propagated to hidden layers and then to output layer. Predicted result is compared to actual result in Energy output. This incur the loss which is exactly squared difference between predicted vs actual energy output. This predictions comes in batches which will sum squared difference between predicted vs actual energy output, incurs loss. So, backward propogation is applied to neural network. Adam Stochastic gradient descent is applied with optimiser to reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss= 'binary_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training CNN on training set and evaluating the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, letâ€™s make this concrete with a small example.\n",
    "\n",
    "Assume you have a dataset with 200 samples (rows of data) and you choose a batch size of 5 and 1,000 epochs.\n",
    "\n",
    "This means that the dataset will be divided into 40 batches, each with five samples. The model weights will be updated after each batch of five samples.\n",
    "\n",
    "This also means that one epoch will involve 40 batches or 40 updates to the model.\n",
    "\n",
    "With 1,000 epochs, the model will be exposed to or pass through the whole dataset 1,000 times. That is a total of 40,000 batches during the entire training process.\n",
    "\n",
    "https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 244s 963ms/step - loss: 0.6926 - accuracy: 0.5279 - val_loss: 0.6030 - val_accuracy: 0.6880\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 228s 911ms/step - loss: 0.6140 - accuracy: 0.6648 - val_loss: 0.5899 - val_accuracy: 0.7140\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 217s 868ms/step - loss: 0.5834 - accuracy: 0.6964 - val_loss: 0.5597 - val_accuracy: 0.7230\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 145s 579ms/step - loss: 0.5540 - accuracy: 0.7189 - val_loss: 0.5223 - val_accuracy: 0.7460\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 142s 567ms/step - loss: 0.5131 - accuracy: 0.7428 - val_loss: 0.4948 - val_accuracy: 0.7645\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 115s 460ms/step - loss: 0.5016 - accuracy: 0.7516 - val_loss: 0.5122 - val_accuracy: 0.7585\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 126s 506ms/step - loss: 0.4627 - accuracy: 0.7791 - val_loss: 0.4834 - val_accuracy: 0.7730\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 134s 535ms/step - loss: 0.4541 - accuracy: 0.7859 - val_loss: 0.4713 - val_accuracy: 0.7695\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 132s 529ms/step - loss: 0.4252 - accuracy: 0.8029 - val_loss: 0.4644 - val_accuracy: 0.7805\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 202s 807ms/step - loss: 0.4089 - accuracy: 0.8078 - val_loss: 0.4694 - val_accuracy: 0.7670\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 109s 437ms/step - loss: 0.4033 - accuracy: 0.8209 - val_loss: 0.4635 - val_accuracy: 0.7915\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 70s 279ms/step - loss: 0.3888 - accuracy: 0.8254 - val_loss: 0.4490 - val_accuracy: 0.7990\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 70s 278ms/step - loss: 0.3834 - accuracy: 0.8275 - val_loss: 0.4894 - val_accuracy: 0.7965\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 69s 278ms/step - loss: 0.3655 - accuracy: 0.8304 - val_loss: 0.4714 - val_accuracy: 0.7945\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 72s 287ms/step - loss: 0.3502 - accuracy: 0.8427 - val_loss: 0.4995 - val_accuracy: 0.7860\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 70s 279ms/step - loss: 0.3380 - accuracy: 0.8458 - val_loss: 0.4454 - val_accuracy: 0.8110\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 70s 278ms/step - loss: 0.3330 - accuracy: 0.8572 - val_loss: 0.4383 - val_accuracy: 0.8075\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 70s 279ms/step - loss: 0.3001 - accuracy: 0.8734 - val_loss: 0.5008 - val_accuracy: 0.7800\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 69s 278ms/step - loss: 0.2940 - accuracy: 0.8745 - val_loss: 0.4523 - val_accuracy: 0.8105\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 69s 277ms/step - loss: 0.2790 - accuracy: 0.8809 - val_loss: 0.4741 - val_accuracy: 0.7975\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 76s 304ms/step - loss: 0.2742 - accuracy: 0.8809 - val_loss: 0.5448 - val_accuracy: 0.7820\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 74s 295ms/step - loss: 0.2595 - accuracy: 0.8918 - val_loss: 0.4846 - val_accuracy: 0.7910\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 70s 281ms/step - loss: 0.2565 - accuracy: 0.8912 - val_loss: 0.5540 - val_accuracy: 0.7825\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 69s 274ms/step - loss: 0.2380 - accuracy: 0.9060 - val_loss: 0.5137 - val_accuracy: 0.8040\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 68s 272ms/step - loss: 0.2257 - accuracy: 0.9044 - val_loss: 0.5143 - val_accuracy: 0.8030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe55743ecd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=train_set, validation_data=test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Making single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result :  [[1.]]\n",
      "Prediction :  dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_2.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "#print(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "#print(test_image)\n",
    "result = cnn.predict(test_image)\n",
    "train_set.class_indices\n",
    "print('result : ', result)\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "\n",
    "print('Prediction : ', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [7 8]]\n",
      "(2, 2)\n",
      "[[[1 2]\n",
      "  [7 8]]]\n",
      "(1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "gfg = np.array([[1, 2], [7, 8]]) \n",
    "print(gfg)\n",
    "print(gfg.shape) \n",
    "  \n",
    "gfg = np.expand_dims(gfg, axis = 0) \n",
    "print(gfg)\n",
    "print(gfg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result :  [[1.]]\n",
      "Prediction :  dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_4.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "#print(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "#print(test_image)\n",
    "result = cnn.predict(test_image)\n",
    "train_set.class_indices\n",
    "print('result : ', result)\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "\n",
    "print('Prediction : ', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
